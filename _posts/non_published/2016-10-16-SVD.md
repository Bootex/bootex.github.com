---
layout: post
title: PCA(Principal Componet Analysis)
published: False
categories:
- Machine learning
tags:
- Machine learning
- Linear algebra
- PCA
- PCR
- Dimentional reduction
---

SVD(Singular Vector Decompositioin) is the cornerstone of matrix computations. Before explanation about SVD, we need to know some basic concepts related to linear algebra.



<!--more-->

## Basic concepts

### Vector norm

By the matrix product rule, the inner product of two n-vectors u,v is the scalar


$$
u^Tv = v^Tu = \sum^n_{i=1}u_iv_i \\
i = 1, \ldots, n\ j=1,\ldots, m.
$$


`Norm`  can be defined in several ways, the most usual, the Euclidean or $$L_2$$ norm of a vector x is given by


$$
||x|| = (x_1^2+\ldots + x_2^2)^{\frac{1}{2}} = (x^Tx)^{\frac{1}{2}}
$$

### Matrix norm

The $$L_2$$ norm of a matrix is defined as the largest semiaxes of the image of the unit ball.


$$
||A||_2 = \max_{||x||_2=1}||Ax||_2
$$


If we denote by $$\sigma$$ the length of the alrgest semi axes, by u the direction of it, and v one of the vectors x by which the maximum is achieved, we can write $$Av = \sigma u.$$





### Theorem

If A is any m * n matrix with real-valued elements, there exist orthonormal matrices.


$$
U = [u_1, \ldots, u_m], V =  [V_1, \ldots, V_n]\\
A=USV^Y = \sum_{i=1}^p\sigma_iu_iv_i^T, 
\\\text{where p = min(m,n) and S is the diagonal matrix with elements}
\\\sigma_1 \ge \sigma_2 \ge \ldots \ge \sigma_p \ge 0
$$






## SVD

